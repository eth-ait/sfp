{
    "exp_name": ["default", "experiment name"],
    "env": ["reach-v2", "environment name"],
    "epochs": [50, "number of epochs"],

    "seed": [0, "seed"],
    "num_threads": [8, "number of threads to use"],
    "num_test_episodes": [10, "number of test episodes"],
    "save_freq": [1, "number of epochs between savings"],
    "debug": [false, "debug mode (logs to tmp)"],
    "gpu": [false, "whether to use cuda acceleration"],

    "steps_per_epoch": [4000, "environment steps per epoch"],
    "start_steps": [10000, "initial steps of random exploration"],
    "update_after": [1000, "number of steps to start training"],
    "update_every": [50, "interval between training runs"],
    "max_ep_len": [1000, "maximum length of an episode"],
    "terminate_on_success": [false, "whether an episode should terminate when successful"],
    "batch_size": [100, "batch size for optimization"],
    "replay_size": [1000000, "size of the experience replay"],
    "gamma": [0.99, "discount rate"],
    "polyak": [0.995, "polyak rate"],
    "alpha": [0.05, "inverse of reward scale"],
    "lr": [0.001, "learning rate for neural networks"],

    "hid": [256, "units per hidden layer"],
    "l": [2, "number of hidden layers"],

    "visual": [false, "moves to visual RL"],
    "fix_env_seed": [false, "whether to  fix goal in sawyer envs"],
    "neg_rew": [false, "sets rewards to -1/0 instead of 0/1"],

    "action_prior": ["none", "action prior"],
    "goal_cond": [true, "enables goal_conditioning"],
    "her": [false, "enables hindsight experience replay"],
    "replay_k": [4, "hindsight replay ratio"],
    "prioritize": [false, "enables PER"],
    "prioritize_alpha": [0.6, "alpha for PER"],
    "prioritize_beta": [0.4, "beta for PER"],
    "prioritize_epsilon": [1e-5, "epsilon for PER"],
    "clip_gradients": [false, "gradient clipping for Q-networks"],

    "sil": [false, "enables SIL"],
    "sil_m": [4, "how many imitation steps for a single RL update"],
    "sil_bs": [100, "SIL batch size"],
    "sil_weight": [1.0, "weight of SIL loss"],
    "sil_value_weight": [0.1, "relative weight of SIL value loss"],

    "n_step_rew": [10, "steps for Q-value backup"],
    "clip_rew": [false, "clip multi-step reward to [0, 1] range"],

    "use_prior": [false, "enables biasing SAC policy with prior trained on primitives"],
    "prior_model": ["flow", "or 'deterministic', 'lscde', 'vae'"],
    "prior_cond": ["action", "or 'state', 'action+state', 'state+goal'"],
    "prior_n_step": [1, "number of steps to condition prior on"],
    "prior_objective": [false, "modifies SAC objective to maximize mixing weight"],
    "prior_initial": [true, "whether to use prior for initial exploration"],
    "prior_schedule": [0, "number of steps for mixing coefficient scheduling"],
    "data_ratio": [1.0, "amount of data used for training prior"],
    "one_step": [false, "enables unconditional action prior"],
    "bc_epochs": [0, "number of behavioral cloning epochs to initialize policy"],
    "use_polyrl": [false, "enables SAC-PolyRL"],
    
    "rand_init_cond": [true, "conditions prior with uniformly sampled variables at beginning of episode"],
    "lambda_param": [false, "whether to learn lambda as a param or NN"],
    "beta": [0.0, "parameter encouraging high mixing weights"],
    "learn_lambda": [true, "if false, does not train lambda"],
    "lambda_init": [0.95, "initialization for lambda"],
    "prior_clamp": [0.0, "restricts support of prior distribution"],
    "prior_smoothing": [0.0, "smooths out prior by mixing with an uniform distribution"],
    "epsilon": [1e-9, "gradient scaling for lambda function"],
    "sparsity": [14.0, "sparsity parameter for mujoco experiments"],
    "bias": [0.1, "state bias for mujoco experiments"],
    "permute_action": [false, "shuffles action dimension in mujoco"],
    "kl_reg": [false, "whether to use soft kl regularization"]
}
